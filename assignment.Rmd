---
title: "Analysis of Fitness Trackers with Barbell Movement"
output:
  html_document:
    fig_height: 9
    fig_width: 9
  pdf_document: default
---

## Introduction

This analysis uses data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants performing barbell lifts correctly and incorrectly in 5 different ways. The data is available from <http://groupware.les.inf.puc-rio.br/har>. The analysis will use the data from the sensors to predict whether the barbell lifts were perfomred correctly or not.

## Reading the Data

There are 19622 cases in the training dataset with 160 variables while the testing dataset has 20 cases with the same 160 variables.

```{r, message=FALSE, warning=FALSE}
training <- read.csv("pml-training.csv")
dim(training)
testing <- read.csv("pml-testing.csv")
dim(testing)
```

## Data Preparation

There are many variables with blanks in the dataset which are set to NA.

```{r, message=FALSE, warning=FALSE}
training[training==""] <- NA
```

There are 100 variables with 19216 NA values which are of no use for the analysis and are removed.

```{r, message=FALSE, warning=FALSE}
training <- training[,colSums(is.na(training))==0]
dim(training)
```

The non-informative columns X, username, the 3 timestamp columns and the 2 window columns are removed.

```{r, message=FALSE, warning=FALSE}
training <- training[,-c(1:7)]
dim(training)
```

## Splitting the Data

The data is now split into two parts, the training data and the test data to be used to measure the out of sample error.

```{r, message=FALSE, warning=FALSE}
library(caret)
set.seed(1)
inTrain <- createDataPartition(training$classe, p=0.60, list=F)
trainData <- training[inTrain, ]
testData <- training[-inTrain, ]
```

## Model Discussion

This problem is a classification problem predicting 5 classes in the data. Classification problems are best dealt with trees, bagged trees or random forests. Since these models are good with non-linear and non-parametric data there is no obvious need for transforming or pre-processing the data - the missing values are also so extreme that an imputation will not improve the situation.

## Classification Tree

As a first step a classification tree is fit with an accuracy of 0.52.

```{r, message=FALSE, warning=FALSE}
set.seed(1)
mod1 <- train(classe ~ ., data=trainData, method="rpart")
mod1
```

The tree shows that `roll_belt`, `pitch_Forearm`, `magnet_dumbbell_y` and `roll_forearm` are the key variables but is still a very simplicstic model.

```{r, message=FALSE, warning=FALSE}
library(rattle)
par(mar=c(1,1,1,1))
fancyRpartPlot(mod1$finalModel)
```

## Bagged Tree

A bagged tree with the default of 10 bootstraps takes much longer to compute but also has a much higher accuracy of 0.97.

```{r, message=FALSE, warning=FALSE}
set.seed(1)
mod2 <- train(classe ~ ., data=trainData, method="treebag")
mod2
```

The Prediction table is as follows:

```{r, message=FALSE, warning=FALSE}
table(trainData$classe,predict(mod2, trainData))
```

## Random Forest

A random forest is the most accurate with 0.99. Here 10-fold corssvalidation was chosen since the sample is large enough and 100 trees were made as this already takes quite some time.

```{r, message=FALSE, warning=FALSE}
set.seed(1)
mod3 <- train(classe ~ ., data=trainData, method="rf",
                 trControl=trainControl(method="cv", 10), ntree=100)
mod3
```

```{r, message=FALSE, warning=FALSE}
table(trainData$classe,predict(mod3, trainData))
```

## Accuracy Measures

So far the accuracy measures have been on the training data. The idea is to check the accuracy andout of sample error on the test dataset. The model that will be used is the random forest as this seems to predict extremely well.

```{r, message=FALSE, warning=FALSE}
set.seed(1)
predict_mod3 <- predict(mod3, testData)
confusionMatrix(testData$classe, predict_mod3)
```

The accuracy on the testing data is **0.9922** and the out of sample error is thus estimated at **0.0078**.

## Predicting for Test Data Set
Now, we apply the model to the original testing data set downloaded from the data source.
```{r, message=FALSE, warning=FALSE}
predict_20 <- predict(mod3, testing[,colnames(trainData)[-53]])
predict_20
```  
